{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.covariance import MinCovDet\n",
    "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "from matplotlib.colors import TwoSlopeNorm, LogNorm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMissing(df):\n",
    "    \"\"\" Show features with missing values \"\"\"\n",
    "    nullOrd = df.isnull().sum().sort_values(ascending=False)\n",
    "    nullOrd = nullOrd[nullOrd > 0]\n",
    "    nullOrd = pd.DataFrame(nullOrd, columns=['TotalNA'])\n",
    "    nullOrd['PropNA'] = nullOrd['TotalNA'] / len(df)\n",
    "    return nullOrd\n",
    "\n",
    "\n",
    "def computeCorrelation(df, p=0.05):\n",
    "    \"\"\" Compute pairwise correlation, p-value and pair counts \"\"\"\n",
    "    correlations = []\n",
    "    for method in ['kendall', kendalltaur_pval, countPair]:\n",
    "        values = df.corr(method=method).stack()\n",
    "        correlations.append(values)\n",
    "    correlations = (\n",
    "        pd.concat(correlations, axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={'level_0': 'feature1',\n",
    "                         'level_1': 'feature2',\n",
    "                         0: 'R', 1: 'p', 2: 'n'}))\n",
    "    correlations['significant'] = correlations['p'] < p\n",
    "    correlations = correlations[correlations['feature1'] != correlations['feature2']]\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def kendalltaur_pval(x,y):\n",
    "    try:\n",
    "        return kendalltau(x,y)[1]\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def countPair(x, y):\n",
    "    \"\"\" Return count of valid pairs (both not nan) \"\"\"\n",
    "\n",
    "    # Indices where both x and y are NOT np.nan\n",
    "    validIndices = np.intersect1d(\n",
    "        np.where(~np.isnan(x)),\n",
    "        np.where(~np.isnan(y)))\n",
    "    return len(validIndices)\n",
    "\n",
    "\n",
    "def plotTargetCorrelation(correlations, feature, out=None):\n",
    "    \"\"\" Plot correlations relative to feature \"\"\"\n",
    "    targetCorr = (\n",
    "        correlations.loc[correlations['feature1'] == feature]\n",
    "        .set_index('feature2'))\n",
    "    targetCorr = targetCorr.sort_values(by=['p'], ascending=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    targetCorr = targetCorr.loc[targetCorr.index != targetCorr['feature1']]\n",
    "    sns.heatmap(pd.DataFrame(targetCorr['R']), yticklabels=1, cmap='bwr',\n",
    "                norm=TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1), ax=ax1)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.tick_params(left=True)\n",
    "    sns.heatmap(pd.DataFrame(targetCorr['p']), yticklabels=1,\n",
    "                cmap='viridis_r', norm=LogNorm(vmax=1), ax=ax2)\n",
    "    ax2.tick_params(left=True)\n",
    "    ax2.set_ylabel('')\n",
    "    fig.tight_layout()\n",
    "    if out is not None:\n",
    "        fig.savefig(out)\n",
    "    return fig, (ax1, ax2)\n",
    "\n",
    "\n",
    "def plotPairwiseCorrelation(correlations, out=None):\n",
    "    \"\"\" Plot pairwise correlation matrix with\n",
    "        output from computeCorrelation() \"\"\"\n",
    "    wideCorr = correlations.pivot(\n",
    "        columns='feature1', index='feature2', values='R')\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(wideCorr, yticklabels=1, cmap='bwr', square=True,\n",
    "                norm=TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1), ax=ax)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_facecolor('lightgrey')\n",
    "    ax.tick_params(left=True)\n",
    "    fig.tight_layout()\n",
    "    if out is not None:\n",
    "        fig.savefig(out)\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'train.csv'\n",
    "test = 'test.csv'\n",
    "index = 'PassengerId'\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = ({\n",
    "    'Survived': bool, \n",
    "    'Pclass':   int, \n",
    "    'Name':     str,\n",
    "    'Sex':      'category',\n",
    "    'Age':      float,\n",
    "    'SibSp':    int,\n",
    "    'Parch':    int,\n",
    "    'Ticket':  'category',\n",
    "    'Fare':     float,\n",
    "    'Cabin':   'category',\n",
    "    'Embarked':'category'\n",
    "})\n",
    "data = pd.read_csv(train, index_col=index, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCorrelations = computeCorrelation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plotPairwiseCorrelation(allCorrelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingVals = showMissing(data)\n",
    "print(missingVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTargetCorrelation(allCorrelations, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'Age'\n",
    "validFeatures = data.select_dtypes(exclude=['float', 'int']).columns\n",
    "for feature in validFeatures:\n",
    "    if feature == reference:\n",
    "        continue\n",
    "    grouping = [group[reference].dropna().values for _, group in data.groupby(feature)]\n",
    "    H, p = stats.kruskal(*grouping)\n",
    "    if not np.isnan(H):\n",
    "        print(feature, H, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Parch vs Survived\n",
    "  - Parch feature indicates number of parents/children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='Parch', hue='Survived', stat='density', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.copy()\n",
    "temp['AgeGroup'] = pd.cut(data['Age'], 5)\n",
    "temp['FamSize'] = temp['Parch'] + temp['SibSp']\n",
    "temp = temp.groupby(['AgeGroup', 'FamSize'])['Survived'].mean().reset_index()\n",
    "temp = temp.pivot(index='AgeGroup', columns='FamSize', values='Survived')\n",
    "sns.heatmap(temp, cmap='Reds_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'] = data['Name'].apply(lambda x: re.split(',|\\.', x)[1].strip())\n",
    "data['Girl'] = (data['Title'] == 'Miss') & (data['Parch'] > 0)\n",
    "def estimateAgeGroup(X):\n",
    "    if (X['Title'] == 'Miss') & (X['Parch'] > 0):\n",
    "        return 'girl'\n",
    "    elif (X['Title'] == 'Master'):\n",
    "        return 'boy'\n",
    "    elif (X['Sex'] == 'male'):\n",
    "        return 'man'\n",
    "    else:\n",
    "        return 'woman'\n",
    "data['estAgeGroup'] = data.apply(estimateAgeGroup, axis=1)\n",
    "data.groupby(['estAgeGroup', 'Pclass'])['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['FamSize'] = data['SibSp'].apply(lambda x: x if x > 0 else 2)\n",
    "data['FamSize'] = np.log(data['SibSp'] + data['Parch'] + 1)\n",
    "feature = 'FamSize'\n",
    "temp = data.groupby([feature])['Survived'].mean().reset_index()\n",
    "sns.lineplot(x=feature, y='Survived', data=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['FamSize'] = data['SibSp'].apply(lambda x: x if x > 0 else 2)\n",
    "data['FamSize'] = (data['SibSp'] + data['Parch']) + (data['Age'] / 70)\n",
    "sns.histplot(x='Age', hue='Survived', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='Parch', y='SibSp', data=data[data['Survived']==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['x2'] = data['Parch'] + data['SibSp'] + 1 + (data['Age'] / data['Age'].max())\n",
    "data['TicketFrequency'] = data.groupby('Ticket')['Ticket'].transform('count')\n",
    "data['FareAdj'] = data['Fare'] / data['TicketFrequency']\n",
    "sns.histplot(x='FareAdj', hue='Pclass', stat='density', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male1Survived = data.loc[(data['Pclass'] == 3) & (data['Survived'] == 0) & (data['Sex'] == 'female')]\n",
    "male1Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://medium.com/analytics-vidhya/scikit-learn-pipelines-with-custom-transformer-a-step-by-step-guide-9b9b886fd2cc\n",
    "        https://stackoverflow.com/questions/48320396/create-a-custom-sklearn-transformermixin-that-transforms-categorical-variables-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer, KBinsDiscretizer, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeCV, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn import set_config\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel, RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Extension of SimpleImputer to optionally impute \n",
    "        values by group and return a pandas dataframe. \"\"\"\n",
    "    \n",
    "    def __init__(self, variable, by=[], strategy='median'): \n",
    "        self.variable = variable\n",
    "        self.by = by\n",
    "        if strategy == 'most_frequent':\n",
    "            self.strategy = lambda x: x.mode().sample(1).values[0]\n",
    "        else:\n",
    "            self.strategy = strategy\n",
    "        self.maps = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store impute for ungrouped data\n",
    "        self.simpleImpute = X[self.variable].agg(self.strategy)\n",
    "        # Store maps for all grouping levels\n",
    "        for i in range(len(self.by), 0, -1):\n",
    "            subBy = self.by[:i]\n",
    "            mapper = X.groupby(subBy)[self.variable].agg(self.strategy)\n",
    "            if i == 1:\n",
    "                mapper = {(k,): v for k, v in mapper.to_dict().items()}\n",
    "            self.maps.append((subBy , mapper))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for (by, mapper) in self.maps:\n",
    "            fillVals = X[by].apply(tuple, axis=1).map(mapper)\n",
    "            X[self.variable] = X[self.variable].fillna(fillVals)\n",
    "            if not X[self.variable].isnull().values.any():\n",
    "                break\n",
    "        else:\n",
    "            # Replace remaining NaN (with ungrouped)\n",
    "            X[self.variable] = X[self.variable].fillna(self.simpleImpute)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer2(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Extension of SimpleImputer to optionally impute \n",
    "        values by group and return a pandas Series. \"\"\"\n",
    "    \n",
    "    def __init__(self, variable, by=[], strategy='median'): \n",
    "        self.variable = variable\n",
    "        self.by = by\n",
    "        if strategy == 'most_frequent':\n",
    "            self.strategy = lambda x: x.mode().sample(1).values[0]\n",
    "        else:\n",
    "            self.strategy = strategy\n",
    "        self.maps = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store impute for ungrouped data\n",
    "        self.simpleImpute = X[self.variable].agg(self.strategy)\n",
    "        # Store maps for all grouping levels\n",
    "        for i in range(len(self.by), 0, -1):\n",
    "            subBy = self.by[:i]\n",
    "            mapper = X.groupby(subBy)[self.variable].agg(self.strategy)\n",
    "            if i == 1:\n",
    "                mapper = {(k,): v for k, v in mapper.to_dict().items()}\n",
    "            self.maps.append((subBy , mapper))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        imputed = X[self.variable]\n",
    "        for (by, mapper) in self.maps:\n",
    "            fillVals = X[by].apply(tuple, axis=1).map(mapper)\n",
    "            imputed = imputed.fillna(fillVals)\n",
    "            if not imputed.isnull().values.any():\n",
    "                break\n",
    "        else:\n",
    "            # Replace remaining NaN (with ungrouped)\n",
    "            imputed = imputed.fillna(self.simpleImpute)\n",
    "        return imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Use for filtering columns by boolean mask \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Dummy transformer \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureImportance(X, y, prePreprocessor, estimator, vline=None):\n",
    "    \"\"\" Run decision tree ensemble method on a preModel \n",
    "        pipline and plot feature importance \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('prePreprocessor', prePreprocessor),\n",
    "        ('selector',        estimator)])\n",
    "    clf = pipeline.fit(X, y)\n",
    "    columnTransformer = (\n",
    "        clf.named_steps['prePreprocessor'].named_steps['columnTransform'])\n",
    "    try:\n",
    "        selector = clf.named_steps['prePreprocessor'].named_steps['selector']\n",
    "    except KeyError:\n",
    "        selector = None\n",
    "    featureNames = getFeatureNames(columnTransformer, selector)\n",
    "    features = (pd.DataFrame(\n",
    "        {'feature': featureNames,\n",
    "         'importance': clf.named_steps['selector'].feature_importances_})\n",
    "        .sort_values(by=['importance'], ascending=False))\n",
    "    \n",
    "    print(f'Total unfiltered features: {len(featureNames)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(y='feature', x='importance', data=features, ax=ax)\n",
    "    if vline is not None:\n",
    "        ax.axvline(vline)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Feature importance')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom imputation and feature engineering \n",
    "        of Titanic dataset \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._imputes = {}\n",
    "        self._models = {}\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        # Engineer features required for imputing fits\n",
    "        X['Title'] = X['Name'].apply(self.getTitle)\n",
    "        X['ageGroup'] = X.apply(self.estimateAgeGroup, axis=1)\n",
    "        self.maxAge = X['Age'].max()\n",
    "        self._imputes['Age'] = GroupImputer2(\n",
    "            'Age', by=['ageGroup', 'Pclass'], strategy='median').fit(X)\n",
    "        # Remove misleading 0 fares (crew) and adjust fare before imputing fit\n",
    "        # Compute true ticket frequency and compute mean per fam size\n",
    "        X['FamSize'] = X['Parch'] + X['SibSp'] + 1\n",
    "        X['TicketFrequency'] = X.groupby('Ticket')['Ticket'].transform('count')\n",
    "        self._models['TicketFrequency'] = LinearRegression().fit(\n",
    "            X['FamSize'].to_frame(), X['TicketFrequency'])\n",
    "        X['Fare'].replace(0, np.nan)\n",
    "        X['FareAdj'] = X['Fare'] / (X['TicketFrequency'] * 10)\n",
    "        self._imputes['FareAdj'] = GroupImputer2(\n",
    "            'FareAdj', by=['Pclass'], strategy='median').fit(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['Title'] = X['Name'].apply(self.getTitle)\n",
    "        X['ageGroup'] = X.apply(self.estimateAgeGroup, axis=1)\n",
    "        X['Age'] = self._imputes['Age'].transform(X)\n",
    "        # Switch crew fares to 0 and correctly impute\n",
    "        X['Fare'].replace(0, np.nan) \n",
    "        X['FamSize'] = X['Parch'] + X['SibSp'] + 1\n",
    "        X['TicketFrequency'] = self._models['TicketFrequency'].predict(\n",
    "            X['FamSize'].to_frame())\n",
    "        X['FareAdj'] = X['Fare'] / (X['TicketFrequency'] * 10)\n",
    "        X['FareAdj'] = self._imputes['FareAdj'].transform(X)\n",
    "        X['normSibp'] = X['SibSp'].apply(lambda x: x if x > 0 else 2)\n",
    "        return X\n",
    "        \n",
    "        \n",
    "    def estimateAgeGroup(self, X):\n",
    "        if (X['Title'] == 'Miss') & (X['Parch'] > 0):\n",
    "            return 'girl'\n",
    "        elif (X['Title'] == 'Master'):\n",
    "            return 'boy'\n",
    "        elif (X['Sex'] == 'male'):\n",
    "            return 'man'\n",
    "        else:\n",
    "            return 'woman'\n",
    "        \n",
    "        \n",
    "    def getTitle(self, x):\n",
    "        \"\"\" Extract title from name \"\"\"\n",
    "        return re.split(',|\\.', x)[1].strip()\n",
    "\n",
    "        \n",
    "    def _titles(self):\n",
    "        return {\n",
    "            \"Ms\":         \"Mrs\",\n",
    "            \"Mr\" :        \"Mr\",\n",
    "            \"Mrs\" :       \"Mrs\",\n",
    "            \"Miss\" :      \"Mrs\",\n",
    "            \"Master\" :    \"Master\"}\n",
    "      \n",
    "        \n",
    "    def _makeCabin(self, X):\n",
    "        \"\"\" Convert cabin number of cabin section \"\"\"\n",
    "        cabins = X['Cabin'].apply(lambda x: x[0]).fillna('Unknown')\n",
    "        return cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureNames(columnTransformer, selector=None):\n",
    "    \"\"\" Extract feature names from column transformer. \n",
    "        If transformers are pipelines then encoding step\n",
    "        should be last step of that pipeline\n",
    "        Ref: https://github.com/scikit-learn/scikit-learn/issues/12525 \n",
    "    \"\"\"\n",
    "    colNames = np.array([])\n",
    "    for tupleTransformer in columnTransformer.transformers_[:-1]:\n",
    "        if isinstance(tupleTransformer[1], Pipeline): \n",
    "            transformer = tupleTransformer[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = tupleTransformer[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError:\n",
    "            names = tupleTransformer[2]\n",
    "        # This is for kBinDiscretizers, which have nbins\n",
    "        if (isinstance(transformer, KBinsDiscretizer)\n",
    "                and transformer.encode != 'ordinal'):\n",
    "            if transformer.encode != 'ordinal':\n",
    "                nbins = transformer.n_bins_\n",
    "                newNames = []\n",
    "                for col, n in zip(names, nBins):\n",
    "                    newNames = [f'{col}-{i}' for i in range(n)]\n",
    "                names = newNames\n",
    "        colNames = np.append(colNames, names)\n",
    "    if selector is not None:\n",
    "        colNames = colNames[selector.get_support()]\n",
    "    return colNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(train, index_col=index, dtype=dtypes)\n",
    "y = X.pop(target)\n",
    "\n",
    "split = train_test_split(X, y, random_state=0, train_size=0.8, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = map(lambda x: x.copy(), split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountTransformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "CatTransformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot' , OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "BinTransformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "FareDiscretizer = Pipeline(steps=[\n",
    "    ('discrete', KBinsDiscretizer(encode='ordinal', strategy='kmeans'))\n",
    "])\n",
    "AgeDiscretizer = Pipeline(steps=[\n",
    "    ('discrete', KBinsDiscretizer(encode='ordinal', strategy='kmeans'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = ([\n",
    "    ('Count',  CountTransformer,  []),\n",
    "    ('Bin',    BinTransformer,    ['Sex']),\n",
    "    ('Fare',   FareDiscretizer,   ['FareAdj']),\n",
    "    ('Age',    AgeDiscretizer,    ['Age']),\n",
    "    ('None',   NoTransformer(),   ['FamSize', 'normSibp']),\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preModel pipeline distinct from modelling step\n",
    "dataEngineering = Pipeline(steps=[\n",
    "    ('engineer',        FeatureEngineer()),\n",
    "    ('columnTransform', preprocessor),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectEstimator = RandomForestClassifier(random_state=1, n_estimators=50, max_features='sqrt')\n",
    "plotFeatureImportance(X_train, y_train, preSelector, selectEstimator, 0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform feature selection\n",
    "  - Combine the preProcess pipeline with feature selector.\n",
    "  - Run feature selection and identify selected features.\n",
    "  - Selected features and passed to parameter hypertuning pipeline (stage 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "nJobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSelector = Pipeline(steps=[\n",
    "    ('preProcess',    dataEngineering),\n",
    "    ('selector',      RFECV(selectEstimator, cv=cv, scoring='accuracy')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data to pipeline\n",
    "featureSelector.fit(X_train, y_train)\n",
    "# Extract columnTransformer and selector to extract feature names\n",
    "columnTransformer = featureSelector.named_steps['preProcess'].named_steps['columnTransform']\n",
    "selector = featureSelector.named_steps['selector']\n",
    "featureNames = getFeatureNames(columnTransformer, selector)\n",
    "# Create dataframe of transformed data\n",
    "transformedDF = pd.DataFrame(\n",
    "    featureSelector.transform(X_valid), \n",
    "    columns=featureNames)\n",
    "transformedDF.head()\n",
    "\n",
    "selectedFeatures = selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullModel = Pipeline(steps=[\n",
    "    ('preProcess',    dataEngineering),\n",
    "    ('featureFilter', FeatureFilter(selectedFeatures)),\n",
    "    ('model',         RandomForestClassifier(random_state=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide list of param dictionaries, because we only want\n",
    "# to search the gamma parameter space for the linear model\n",
    "params =([\n",
    "    {'preProcess__columnTransform__Fare__discrete__n_bins': Integer(2, 6),\n",
    "     'preProcess__columnTransform__Age__discrete__n_bins': Integer(2, 6),\n",
    "     'model__n_estimators':      Integer(10, 1000),\n",
    "     'model__max_depth':         Integer(3, 20),\n",
    "     'model__max_features':      Categorical(['sqrt']),\n",
    "     'model__criterion':         Categorical(['gini'])},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch = BayesSearchCV(\n",
    "    fullModel, params, scoring='accuracy',\n",
    "    cv=cv, refit=True, n_jobs=nJobs, n_iter=100)\n",
    "gridSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = gridSearch.score(X_valid, y_valid)\n",
    "print(f'Best score: {score}')\n",
    "\n",
    "gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions\n",
    " - Refit model with full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridSearch.fit(X, y)\n",
    "X_test = pd.read_csv(test, index_col=index, dtype=dtypes)\n",
    "predictions = gridSearch.predict(X_test).astype(int)\n",
    "submission = pd.DataFrame({'PassengerId':X_test.index,'Survived': predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation - assessing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preModelTransformer = gridSearch.best_estimator_.named_steps['preModel']\n",
    "columnTransformer = preModelTransformer.named_steps['columnTransform']\n",
    "model = gridSearch.best_estimator_.named_steps['model']\n",
    "featureNames = getFeatureNames(columnTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the preModelTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedDf = pd.DataFrame(preModelTransformer.transform(X_valid), columns=featureNames)\n",
    "transformedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(model, feature_names=featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "sns.barplot(y='Fare', x='FamilySize', hue='Pclass', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FareAdj'] = data['Fare'] / data['FamilySize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data[data['Pclass']==3], x='FareAdj', hue='Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(x='x2', hue='Survived', kind='kde', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard encoded rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleAugmentedEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Augments sklearn estimators with rule-based logic.\n",
    "        This class is a wrapper class for sklearn estimators with the additional\n",
    "    possibility of adding rule-based logic to the underlying estimator.\n",
    "    The provided rules are hard-coded and take precedence over the underlying\n",
    "    estimator's predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model: BaseEstimator, **baseParams):  \n",
    "        self.base_model = base_model\n",
    "        self.base_model.set_params(**baseParams)\n",
    "\n",
    "   \n",
    "    def _get_base_model_data(self, X, y):\n",
    "        \"\"\"Filters the trainig data for data points not affected by the rules.\"\"\"\n",
    "        \n",
    "        train_x = X\n",
    "        male23 = (train_x['Sex'] == 'male') & (train_x['Pclass'] != 1)\n",
    "        fem1 = (train_x['Sex'] == 'female') & (train_x['Pclass'] == 2)\n",
    "        mask = [any(tup) for tup in zip(male23, fem1)]\n",
    "        train_x = train_x.loc[mask]\n",
    "        train_y = y.loc[mask]\n",
    "        \n",
    "        train_x = train_x.reset_index(drop=True)\n",
    "        train_y = train_y.reset_index(drop=True)\n",
    "        \n",
    "        return train_x, train_y   \n",
    "\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        train_x, train_y = self._get_base_model_data(X, y)\n",
    "        self.base_model.fit(train_x, train_y, **kwargs)\n",
    "    \n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.array:\n",
    "        \"\"\"Gets predictions for the provided feature data.\n",
    "        \n",
    "        The predicitons are evaluated using the provided rules wherever possible\n",
    "        otherwise the underlying estimator is used.\n",
    "        \n",
    "        Args:\n",
    "            X: The feature data to evaluate predictions for.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Evaluated predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        p_X = X.copy()\n",
    "        p_X['prediction'] = np.nan\n",
    "        p_X.loc[(p_X['Sex'] == 'male') & (p_X['Pclass'] != 1), 'prediction'] = False\n",
    "        p_X.loc[(p_X['Sex'] == 'female') & (p_X['Pclass'] == 1), 'prediction'] = True\n",
    "        \n",
    "        if len(p_X.loc[p_X['prediction'].isna()].index != 0):\n",
    "            base_X = p_X.loc[p_X['prediction'].isna()].copy()\n",
    "            base_X.drop('prediction', axis=1, inplace=True)\n",
    "            p_X.loc[p_X['prediction'].isna(), 'prediction'] = self.base_model.predict(base_X)\n",
    "        return list(p_X['prediction'])\n",
    "    \n",
    "    \n",
    "    def get_params(self, deep: bool = True):\n",
    "        return self.base_model.get_params(deep=deep)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
