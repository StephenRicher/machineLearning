{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.covariance import MinCovDet\n",
    "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "from matplotlib.colors import TwoSlopeNorm, LogNorm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMissing(df):\n",
    "    \"\"\" Show features with missing values \"\"\"\n",
    "    nullOrd = df.isnull().sum().sort_values(ascending=False)\n",
    "    nullOrd = nullOrd[nullOrd > 0]\n",
    "    nullOrd = pd.DataFrame(nullOrd, columns=['TotalNA'])\n",
    "    nullOrd['PropNA'] = nullOrd['TotalNA'] / len(X)\n",
    "    return nullOrd\n",
    "\n",
    "\n",
    "def computeCorrelation(df, p=0.05):\n",
    "    \"\"\" Compute pairwise correlation, p-value and pair counts \"\"\"\n",
    "    correlations = []\n",
    "    for method in ['kendall', kendalltaur_pval, countPair]:\n",
    "        values = df.corr(method=method).stack()\n",
    "        correlations.append(values)\n",
    "    correlations = (\n",
    "        pd.concat(correlations, axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={'level_0': 'feature1',\n",
    "                         'level_1': 'feature2',\n",
    "                         0: 'R', 1: 'p', 2: 'n'}))\n",
    "    correlations['significant'] = correlations['p'] < p\n",
    "    correlations = correlations[correlations['feature1'] != correlations['feature2']]\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def kendalltaur_pval(x,y):\n",
    "    try:\n",
    "        return kendalltau(x,y)[1]\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def countPair(x, y):\n",
    "    \"\"\" Return count of valid pairs (both not nan) \"\"\"\n",
    "\n",
    "    # Indices where both x and y are NOT np.nan\n",
    "    validIndices = np.intersect1d(\n",
    "        np.where(~np.isnan(x)),\n",
    "        np.where(~np.isnan(y)))\n",
    "    return len(validIndices)\n",
    "\n",
    "\n",
    "def plotTargetCorrelation(correlations, feature, out=None):\n",
    "    \"\"\" Plot correlations relative to feature \"\"\"\n",
    "    targetCorr = (\n",
    "        correlations.loc[correlations['feature1'] == feature]\n",
    "        .set_index('feature2'))\n",
    "    targetCorr = targetCorr.sort_values(by=['p'], ascending=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    targetCorr = targetCorr.loc[targetCorr.index != targetCorr['feature1']]\n",
    "    sns.heatmap(pd.DataFrame(targetCorr['R']), yticklabels=1, cmap='bwr',\n",
    "                norm=TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1), ax=ax1)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.tick_params(left=True)\n",
    "    sns.heatmap(pd.DataFrame(targetCorr['p']), yticklabels=1,\n",
    "                cmap='viridis_r', norm=LogNorm(vmax=1), ax=ax2)\n",
    "    ax2.tick_params(left=True)\n",
    "    ax2.set_ylabel('')\n",
    "    fig.tight_layout()\n",
    "    if out is not None:\n",
    "        fig.savefig(out)\n",
    "    return fig, (ax1, ax2)\n",
    "\n",
    "\n",
    "def plotPairwiseCorrelation(correlations, out=None):\n",
    "    \"\"\" Plot pairwise correlation matrix with\n",
    "        output from computeCorrelation() \"\"\"\n",
    "    wideCorr = correlations.pivot(\n",
    "        columns='feature1', index='feature2', values='R')\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(wideCorr, yticklabels=1, cmap='bwr', square=True,\n",
    "                norm=TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1), ax=ax)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_facecolor('lightgrey')\n",
    "    ax.tick_params(left=True)\n",
    "    fig.tight_layout()\n",
    "    if out is not None:\n",
    "        fig.savefig(out)\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'train.csv'\n",
    "test = 'test.csv'\n",
    "index = 'PassengerId'\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = ({\n",
    "    'Survived': bool, \n",
    "    'Pclass':   int, \n",
    "    'Name':     str,\n",
    "    'Sex':      'category',\n",
    "    'Age':      float,\n",
    "    'SibSp':    int,\n",
    "    'Parch':    int,\n",
    "    'Ticket':  'category',\n",
    "    'Fare':     float,\n",
    "    'Cabin':   'category',\n",
    "    'Embarked':'category'\n",
    "})\n",
    "X = pd.read_csv(train, index_col=index, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ordinal encoding here if necessary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCorrelations = computeCorrelation(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plotPairwiseCorrelation(allCorrelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingVals = showMissing(X)\n",
    "print(missingVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTargetCorrelation(allCorrelations, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'Age'\n",
    "validFeatures = X.select_dtypes(exclude=['float', 'int']).columns\n",
    "for feature in validFeatures:\n",
    "    if feature == reference:\n",
    "        continue\n",
    "    grouping = [group[reference].dropna().values for _, group in X.groupby(feature)]\n",
    "    H, p = stats.kruskal(*grouping)\n",
    "    if not np.isnan(H):\n",
    "        print(feature, H, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://medium.com/analytics-vidhya/scikit-learn-pipelines-with-custom-transformer-a-step-by-step-guide-9b9b886fd2cc\n",
    "        https://stackoverflow.com/questions/48320396/create-a-custom-sklearn-transformermixin-that-transforms-categorical-variables-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeCV, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn import set_config\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Extension of SimpleImputer to impute values by group \"\"\"\n",
    "    \n",
    "    def __init__(self, variable, by, strategy='median'): \n",
    "        self.variable = variable\n",
    "        self.by = by\n",
    "        if strategy == 'most_frequent':\n",
    "            self.strategy = lambda x: x.mode().sample(1).values[0]\n",
    "        else:\n",
    "            self.strategy = strategy\n",
    "        self.maps = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store impute for ungrouped data\n",
    "        self.simpleImpute = X[self.variable].agg(self.strategy)\n",
    "        # Store maps for all grouping levels\n",
    "        for i in range(len(self.by), 0, -1):\n",
    "            subBy = self.by[:i]\n",
    "            mapper = X.groupby(subBy)[self.variable].agg(self.strategy)\n",
    "            if i == 1:\n",
    "                mapper = {(k,): v for k, v in mapper.to_dict().items()}\n",
    "            self.maps.append((subBy , mapper))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for (by, mapper) in self.maps:\n",
    "            fillVals = X[by].apply(tuple, axis=1).map(mapper)\n",
    "            X[self.variable] = X[self.variable].fillna(fillVals)\n",
    "            if not X[self.variable].isnull().values.any():\n",
    "                break\n",
    "        else:\n",
    "            # Replace remaining NaN (with ungrouped)\n",
    "            X[self.variable] = X[self.variable].fillna(self.simpleImpute)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Engineer custom features \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['Relatives'] = X['SibSp'] + X['Parch']\n",
    "        X['Title'] = X['Name'].apply(lambda x: re.split(',|\\.', x)[1].strip())\n",
    "        X['Title'] = X['Title'].map(self._titles())\n",
    "        X['Cabin'] = X['Cabin'].apply(lambda x: x[0]).map(self._cabins())\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def _titles(self):\n",
    "        return {\n",
    "            \"Capt\":       \"Officer\",\n",
    "            \"Col\":        \"Officer\",\n",
    "            \"Major\":      \"Officer\",\n",
    "            \"Jonkheer\":   \"Royalty\",\n",
    "            \"Don\":        \"Royalty\",\n",
    "            \"Sir\" :       \"Royalty\",\n",
    "            \"Dr\":         \"Officer\",\n",
    "            \"Rev\":        \"Officer\",\n",
    "            \"the Countess\":\"Royalty\",\n",
    "            \"Dona\":       \"Royalty\",\n",
    "            \"Mme\":        \"Mrs\",\n",
    "            \"Mlle\":       \"Miss\",\n",
    "            \"Ms\":         \"Mrs\",\n",
    "            \"Mr\" :        \"Mr\",\n",
    "            \"Mrs\" :       \"Mrs\",\n",
    "            \"Miss\" :      \"Miss\",\n",
    "            \"Master\" :    \"Master\",\n",
    "            \"Lady\" :      \"Royalty\"}\n",
    "\n",
    "    def _cabins(self):\n",
    "        return {\n",
    "            'A': 1, 'B': 2, 'C': 3, 'D': 4,\n",
    "            'E': 5, 'F': 6, 'G': 7, 'T': 8 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Dummy transformer \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureNames(columnTransformer):\n",
    "    \"\"\" Extract feature names from column transformer. \n",
    "        If transformers are pipelines then encoding step\n",
    "        should be last step of that pipeline\n",
    "        Ref: https://github.com/scikit-learn/scikit-learn/issues/12525 \n",
    "    \"\"\"\n",
    "    colNames = []\n",
    "    for tupleTransformer in columnTransformer.transformers_[:-1]:\n",
    "        if isinstance(tupleTransformer[1], Pipeline): \n",
    "            transformer = tupleTransformer[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = tupleTransformer[1]\n",
    "        try:\n",
    "            names = transformer.get_feature_names()\n",
    "        except AttributeError:\n",
    "            names = tupleTransformer[2]\n",
    "        if isinstance(names, str):\n",
    "            colNames.append(names)\n",
    "        else:\n",
    "            colNames.extend(list(names))   \n",
    "    return colNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureImportance(X, y, preModel, estimator):\n",
    "    \"\"\" Run decision tree ensemble method on a preModel \n",
    "        pipline and plot feature importance \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preModel', preModel),\n",
    "        ('model',    estimator)])\n",
    "    clf = pipeline.fit(X, y)\n",
    "    columnTransformer = (\n",
    "        clf.named_steps['preModel'].named_steps['columnTransform'])\n",
    "    featureNames = getFeatureNames(columnTransformer)\n",
    "    features = (pd.DataFrame(\n",
    "        {'feature': getFeatureNames(columnTransformer),\n",
    "         'importance': clf.named_steps['model'].feature_importances_})\n",
    "        .sort_values(by=['importance'], ascending=False))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(y='feature', x='importance', data=features, ax=ax)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Feature importance')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(train, index_col=index, dtype=dtypes)\n",
    "y = X.pop(target)\n",
    "\n",
    "split = train_test_split(X, y, random_state=0, train_size=0.8, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = map(lambda x: x.copy(), split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountTransformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "CatTransformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot' , OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "FareTransformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  RobustScaler()),\n",
    "    ('power',   PowerTransformer(method='yeo-johnson'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = ([\n",
    "    ('Count',  CountTransformer,  ['Pclass', 'Relatives']),\n",
    "    ('Cat',    CatTransformer,    ['Sex', 'Embarked']),\n",
    "    ('Fare',   FareTransformer,   ['Fare']),\n",
    "    ('None',   NoTransformer(),   ['Age', 'Cabin']),\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preModel pipeline distinct from modelling step\n",
    "preModel = Pipeline(steps=[\n",
    "    ('engineer',        FeatureEngineer()),\n",
    "    ('ageImputer',      GroupImputer('Age', by=['Title', 'Sex'], strategy='median')),\n",
    "    ('cabinImputer',    GroupImputer('Cabin', by=['Pclass'], strategy='most_frequent')),\n",
    "    ('columnTransform', preprocessor),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(random_state=1, n_estimators=50, max_features='sqrt')\n",
    "plotFeatureImportance(X_train, y_train, preModel, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total workflow pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preModel',  preModel),\n",
    "    ('selection', SelectFromModel(estimator, max_features=1)),\n",
    "    ('model',     XGBClassifier(random_state=1))\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "nJobs = 4\n",
    "\n",
    "# Provide list of param dictionaries, because we only want\n",
    "# to search the gamma parameter space for the linear model\n",
    "params =([\n",
    "    {'model__n_estimators': Integer(10, 100),\n",
    "     'model__max_depth':   Integer(3, 20)},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch = BayesSearchCV(\n",
    "    pipeline, params, scoring='accuracy',\n",
    "    cv=cv, refit=True, n_jobs=nJobs, n_iter=1, verbose=10)\n",
    "gridSearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions\n",
    " - Refit model with full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch.fit(X, y)\n",
    "X_test = pd.read_csv(test, index_col=index, dtype=dtypes)\n",
    "predictions = gridSearch.predict(X_test).astype(int)\n",
    "submission = pd.DataFrame({'PassengerId':X_test.index,'Survived': predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation - assessing feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preModelTransformer = gridSearch.best_estimator_.named_steps['preModel']\n",
    "columnTransformer = preModelTransformer.named_steps['columnTransform']\n",
    "model = gridSearch.best_estimator_.named_steps['model']\n",
    "featureNames = getFeatureNames(columnTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the preModelTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedDf = pd.DataFrame(preModelTransformer.transform(X_valid), columns=featureNames)\n",
    "transformedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(model, feature_names=featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Age'].agg('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
