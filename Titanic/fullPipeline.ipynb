{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import phik\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 7, 3)\n",
    "assert sklearn.__version__ == '0.23.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "# Display estimators as diagrams in Jupyter notebook\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to datasets and target/feature information\n",
    "trainPath = 'train.csv'\n",
    "testPath = 'test.csv'\n",
    "target = 'Survived'\n",
    "index = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicity define datatypes\n",
    "dtypes = ({\n",
    "    'Survived': bool, \n",
    "    'Pclass':   int, \n",
    "    'Name':     'category',\n",
    "    'Sex':      'category',\n",
    "    'Age':      float,\n",
    "    'SibSp':    int,\n",
    "    'Parch':    int,\n",
    "    'Ticket':  'category',\n",
    "    'Fare':     float,\n",
    "    'Cabin':   'category',\n",
    "    'Embarked':'category'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    " - Here we explore the training dataset and relationships between features.\n",
    " - We also explore possible new features to engineer.\n",
    " - Test dataset is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(trainPath, index_col=index, dtype=dtypes)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View missing data\n",
    " - We will drop Cabin due to proportion of missing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMissing(df):\n",
    "    \"\"\" Show features with missing values \"\"\"\n",
    "    nullOrd = df.isnull().sum().sort_values(ascending=False)\n",
    "    nullOrd = nullOrd[nullOrd > 0]\n",
    "    nullOrd = pd.DataFrame(nullOrd, columns=['TotalNA'])\n",
    "    nullOrd['PropNA'] = nullOrd['TotalNA'] / len(df)\n",
    "    return nullOrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(showMissing(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Cabin feature\n",
    "train = train.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial feature engineering\n",
    " - Prior to visualising the data there are some additional features we can extract based on our knowledge of the data.\n",
    "   - **Family Size** (from Parch and SibSp)\n",
    "     - The sum of Parch and SibSp + 1 gives total family size.\n",
    "   - **Surname** (from Name)\n",
    "     - Indicates family grouping.\n",
    "   - **Title** (from Name)\n",
    "     - Contains information on age, sex, social status.\n",
    "   - **Woman or Child** (from Title and Sex)\n",
    "     - We can engineer this group without using Age since boys have the title 'Master'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FamSize'] = train['Parch'] + train['SibSp'] + 1\n",
    "train['Surname'] = train['Name'].apply(lambda x: re.split(',', x)[0].strip())\n",
    "train['Title'] = train['Name'].apply(lambda x: re.split(',|\\.', x)[1].strip())\n",
    "train['womanOrChild'] = (train['Sex'] == 'female') | (train['Title'] == 'Master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check frequency of each title - 4 titles dominate so we will group all others together for now.\n",
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean mask of any title NOT in list\n",
    "otherTitles = ~train['Title'].isin(['Mr', 'Miss', 'Mrs', 'Master'])\n",
    "# Replace non-standard titles with other to reduce cardinality.\n",
    "train.loc[otherTitles, 'Title'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family Survival\n",
    " - Here we create 2 boolean features to quantify family survive for a given surname.\n",
    " - Since most males dies, if a male of the family survived the rest of his family may have survived to.\n",
    " - Since most women and children survived, if they died then the rest of their family may have died also.\n",
    "  - Adult male survived.\n",
    "  - Female or boy (title = 'Master') died."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean feature describing if family has a surviving adult male\n",
    "maleSurviveWithFam = (\n",
    "    (train['Sex'] == 'male') & (train['Title'] != 'Master') & \n",
    "    (train['Survived'] == 1) & (train['FamSize'] > 1))\n",
    "maleNames = train.loc[maleSurviveWithFam, 'Surname']\n",
    "train['famSurvive'] = train['Surname'].isin(maleNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean feature describing if family has a dead women/child\n",
    "womenChildDieWithFam = (\n",
    "    ((train['Sex'] == 'female') | (train['Title'] == 'Master')) &\n",
    "    (train['Survived'] == 0) & (train['FamSize'] > 1))\n",
    "womenChildNames = train.loc[womenChildDieWithFam, 'Surname']\n",
    "train['famDie'] = train['Surname'].isin(womenChildNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cardinality of non-numeric groups\n",
    "  - High cardinality groups will be dropped for Phik analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkCardinality(df):\n",
    "    \"\"\" Return number of unique groups \n",
    "        for non-numeric columns \"\"\"\n",
    "    cardinality = (\n",
    "        df\n",
    "        .select_dtypes(exclude='number')\n",
    "        .apply(pd.Series.nunique)\n",
    "        .sort_values(ascending=False))\n",
    "    return cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkCardinality(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMatrix(df, sortOn=None, mask=None):\n",
    "    \"\"\" Wrapper for plotting phik matrix \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    if sortOn is not None:\n",
    "        # Retrieve sorted columns\n",
    "        order = df[target].sort_values(ascending=False).index\n",
    "        # Reorder rows and columns\n",
    "        phik = df.reindex(order)[order]\n",
    "    # Plot heatmap\n",
    "    heatmap = sns.heatmap(df, cmap='Reds', vmin=0, vmax=1, mask=mask, ax=ax)\n",
    "    heatmap.set_facecolor('grey')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalCols = ['Age', 'Fare']\n",
    "dropCols = ['Name', 'Ticket', 'Surname']\n",
    "phikMatrix = train.drop(dropCols, axis=1).phik_matrix(interval_cols=intervalCols)\n",
    "sigMatrix = train.drop(dropCols, axis=1).significance_matrix(interval_cols=intervalCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plotMatrix(phikMatrix, sortOn=target, mask=(phikMatrix == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phikNoTarget = 1 - phikMatrix.drop(target, axis=0).drop(target, axis=1)\n",
    "phikClusterer = AgglomerativeClustering(\n",
    "    affinity='precomputed', linkage='average', \n",
    "    n_clusters=None, distance_threshold=0.25)\n",
    "\n",
    "# Associate cluster labels with features names\n",
    "labelledClusters = (pd.DataFrame(\n",
    "    {'cluster': phikClusterer.fit(phikNoTarget).labels_, \n",
    "     'feature': phikNoTarget.index}))\n",
    "\n",
    "# Add association with target and sort\n",
    "labelledClusters = (\n",
    "    pd.merge(phikMatrix[target].drop(target), labelledClusters, \n",
    "             left_index=True, right_on='feature')\n",
    "    .sort_values(target, ascending=False))\n",
    "    \n",
    "# Group by cluster and print clustered features.\n",
    "# Features are ordered in a cluster according to their association with the target\n",
    "# and cluster are order by their mean association with target.\n",
    "clusterFeatures = labelledClusters.groupby('cluster')\n",
    "clusterFeatures = (\n",
    "    pd.concat([clusterFeatures['feature'].apply(list), \n",
    "               clusterFeatures[target].mean()], axis=1)\n",
    "    .sort_values('Survived', ascending=False))\n",
    "print(clusterFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model\n",
    " - Here we build a custom transformer incorporating all of the preprocessing steps.\n",
    "  - Using a custom transformer within a pipeline helps prevent data leakage and make it easy to run the pipeline on test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Extension of SimpleImputer to optionally impute \n",
    "        values by group and return a pandas Series. \"\"\"\n",
    "    \n",
    "    def __init__(self, variable, by=[], strategy='median'): \n",
    "        self.variable = variable\n",
    "        self.by = by\n",
    "        if strategy == 'most_frequent':\n",
    "            self.strategy = lambda x: x.mode().sample(1).values[0]\n",
    "        else:\n",
    "            self.strategy = strategy\n",
    "        self.maps = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Store impute for ungrouped data\n",
    "        self.simpleImpute = X[self.variable].agg(self.strategy)\n",
    "        # Store maps for all grouping levels\n",
    "        for i in range(len(self.by), 0, -1):\n",
    "            subBy = self.by[:i]\n",
    "            mapper = X.groupby(subBy)[self.variable].agg(self.strategy)\n",
    "            if i == 1:\n",
    "                mapper = {(k,): v for k, v in mapper.to_dict().items()}\n",
    "            self.maps.append((subBy , mapper))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        imputed = X[self.variable]\n",
    "        for (by, mapper) in self.maps:\n",
    "            fillVals = X[by].apply(tuple, axis=1).map(mapper)\n",
    "            imputed = imputed.fillna(fillVals)\n",
    "            if not imputed.isnull().values.any():\n",
    "                break\n",
    "        else:\n",
    "            # Replace remaining NaN (with ungrouped)\n",
    "            imputed = imputed.fillna(self.simpleImpute)\n",
    "        return imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NoTransformer\n",
    " - In this analysis we make use of the ColumnTransformer to explicity define a pre-processing step for each feature.\n",
    " - Any features not included in the ColumnTransformer are dropped.\n",
    " - This transformer allows us to pass the data through the ColumnTransformer and retain features that do not need pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Dummy transformer \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom imputation and feature engineering \n",
    "        of Titanic dataset \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._imputes = {}\n",
    "        \n",
    "\n",
    "    def initData(self, X):\n",
    "        \"\"\" Feature engineering required for both fit & transform \"\"\"\n",
    "        X['Title'] = X['Name'].apply(self.getTitle)\n",
    "        X['womanOrChild'] = (X['Sex'] == 'female') | (X['Title'] == 'Master')\n",
    "        X['Surname'] = X['Name'].apply(self.getSurname)\n",
    "        X['FamSize'] = X['Parch'] + X['SibSp'] + 1\n",
    "        X['ageGroup'] = X.apply(self.estimateAgeGroup, axis=1)\n",
    "\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.initData(X)\n",
    "        # Store surnames of all adult males (with families) that survived\n",
    "        maleSurviveWithFam = (\n",
    "            (X['Sex'] == 'male') & (X['Title'] != 'Master') & \n",
    "            (y == 1) & (X['FamSize'] > 1))\n",
    "        self.maleNames = X.loc[maleSurviveWithFam, 'Surname']\n",
    "        # Store surnames of all women and children (with families) that died\n",
    "        femaleDieWithFam = (\n",
    "            ((X['Sex'] == 'female') | (X['Title'] == 'Master')) &\n",
    "            (y == 0) & (X['FamSize'] > 1))\n",
    "        # Store surnames of all females (with families) that died\n",
    "        self.femaleNames = X.loc[femaleDieWithFam, 'Surname']\n",
    "        self._imputes['Age'] = GroupImputer(\n",
    "            'Age', by=['ageGroup', 'Pclass'], strategy='median').fit(X)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.initData(X)\n",
    "        X['Age'] = self._imputes['Age'].transform(X)\n",
    "        X['famSurvive'] = X['Surname'].isin(self.maleNames)\n",
    "        X['famDie'] = X['Surname'].isin(self.femaleNames)\n",
    "        return X\n",
    "\n",
    "    \n",
    "    def estimateAgeGroup(self, X):\n",
    "        \"\"\" Estimate age/sex group by title for age imputation \"\"\"\n",
    "        # Assume unmarried with parents is a girl\n",
    "        if (X['Title'] == 'Miss') & (X['Parch'] > 0):\n",
    "            return 'girl'\n",
    "        elif (X['Title'] == 'Master'):\n",
    "            return 'boy'\n",
    "        elif (X['Sex'] == 'male'):\n",
    "            return 'man'\n",
    "        else:\n",
    "            return 'woman'    \n",
    "        \n",
    "        \n",
    "    def getTitle(self, x):\n",
    "        \"\"\" Extract title from name \"\"\"\n",
    "        return re.split(',|\\.', x)[1].strip()\n",
    "    \n",
    "    \n",
    "    def getSurname(self, x):\n",
    "        \"\"\" Extract surname from name \"\"\"\n",
    "        return re.split(',', x)[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data\n",
    " - Seperate feature matrix and target vector.\n",
    " - Split into training and validation datasets.\n",
    "   - Since test_train_split returns a view we run the result through map() to yeild a true copy of each.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(trainPath, index_col=index, dtype=dtypes)\n",
    "y = X.pop(target)\n",
    "\n",
    "split = train_test_split(X, y, random_state=0, train_size=0.8, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = map(lambda x: x.copy(), split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbarkedTransformer = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encode', OneHotEncoder(\n",
    "        handle_unknown='ignore', categories=[['C', 'Q', 'S']]))\n",
    "])\n",
    "AgeDiscretizer = Pipeline(steps=[\n",
    "    ('discrete', KBinsDiscretizer(encode='ordinal', strategy='kmeans'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = ([\n",
    "    ('Pclass',   OneHotEncoder(handle_unknown='ignore', categories=[[1, 2, 3]]),  ['Pclass']),\n",
    "    ('Embarked', EmbarkedTransformer, ['Embarked']),\n",
    "    ('Age',      AgeDiscretizer, ['Age']),\n",
    "    ('None',     NoTransformer(), ['famDie', 'famSurvive', 'womanOrChild', 'FamSize']),\n",
    "])\n",
    "featureTransformer = ColumnTransformer(transformers=transformers, remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preModel pipeline distinct from modelling step\n",
    "preProcessor = Pipeline(steps=[\n",
    "    ('engineer',        FeatureEngineer()),\n",
    "    ('columnTransform', featureTransformer),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess feature importance\n",
    " - This function takes a preProcessor pipeline and a tree estimator to assess feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureImportance(X, y, prePreprocessor, estimator, vline=None):\n",
    "    \"\"\" Run decision tree ensemble method on a preModel \n",
    "        pipline and plot feature importance \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('prePreprocessor', prePreprocessor),\n",
    "        ('selector',        estimator)])\n",
    "    clf = pipeline.fit(X, y)\n",
    "    columnTransformer = (\n",
    "        clf.named_steps['prePreprocessor'].named_steps['columnTransform'])\n",
    "    try:\n",
    "        selector = clf.named_steps['prePreprocessor'].named_steps['selector']\n",
    "    except KeyError:\n",
    "        selector = None\n",
    "    featureNames = getFeatureNames(columnTransformer, selector)\n",
    "    features = (pd.DataFrame(\n",
    "        {'feature': featureNames,\n",
    "         'importance': clf.named_steps['selector'].feature_importances_})\n",
    "        .sort_values(by=['importance'], ascending=False))\n",
    "    \n",
    "    print(f'Total unfiltered features: {len(featureNames)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(y='feature', x='importance', data=features, ax=ax)\n",
    "    if vline is not None:\n",
    "        ax.axvline(vline)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Feature importance')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureNames(columnTransformer, selector=None):\n",
    "    \"\"\" Extract feature names from column transformer object. \n",
    "        If transformers are pipelines the one-hot encoding step\n",
    "        should be last step of that pipeline.\n",
    "    \"\"\"\n",
    "    colNames = np.array([])\n",
    "    for tupleTransformer in columnTransformer.transformers_[:-1]:\n",
    "        if isinstance(tupleTransformer[1], Pipeline): \n",
    "            transformer = tupleTransformer[1].steps[-1][1]\n",
    "        else:\n",
    "            transformer = tupleTransformer[1]\n",
    "        try:\n",
    "            # One hot encoded names have x0_, x1_ etc.\n",
    "            names = transformer.get_feature_names()\n",
    "            trueNames = tupleTransformer[-1]\n",
    "            # Get dict mapping transformed name to true name\n",
    "            nameMap = {f'x{i}_' : name for i, name in enumerate(trueNames)}\n",
    "            # Swap transformed name with true name\n",
    "            for i, name in enumerate(names):\n",
    "                prefix = name[:3]\n",
    "                names[i] = f'{nameMap[prefix]}_{name[3:]}'\n",
    "        except AttributeError:\n",
    "            names = tupleTransformer[2]\n",
    "        # This is for kBinDiscretizers, which have n_bins_ method\n",
    "        if (isinstance(transformer, KBinsDiscretizer)\n",
    "                and transformer.encode != 'ordinal'):\n",
    "            if transformer.encode != 'ordinal':\n",
    "                nBins = transformer.n_bins_\n",
    "                newNames = []\n",
    "                for col, n in zip(names, nBins):\n",
    "                    newNames = [f'{col}-{i}' for i in range(n)]\n",
    "                names = newNames\n",
    "        colNames = np.append(colNames, names)\n",
    "    if selector is not None:\n",
    "        colNames = colNames[selector.get_support()]\n",
    "    return colNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the plotFeatureImportance function assumes the columnTransformer is named 'columnTransform'\n",
    "# Future update could loop through steps and find the ColumnTransformer object\n",
    "selectEstimator = RandomForestClassifier(random_state=1, n_estimators=500, max_features='sqrt')\n",
    "plotFeatureImportance(X_train, y_train, preProcessor, selectEstimator, 0.015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform recursive feature elimination\n",
    "  - Combine the preProcessor pipeline with feature selector.\n",
    "  - Run feature selection and identify selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the cross-validation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "nJobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSelector = Pipeline(steps=[\n",
    "    ('preProcess',    preProcessor),\n",
    "    ('selector',      RFECV(selectEstimator, cv=cv, scoring='accuracy')),\n",
    "])\n",
    "# Fit data to pipeline\n",
    "featureSelector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View transformed data\n",
    " - Here we view our transformed and feature filtered dataset as a sanity check prior to running the model.\n",
    " - The custom function 'getFeatureNames' processes a columnTransformer object to extract the original feature names.\n",
    "  - This allows us to view the processed numpy matrix as a labelled dataframe.\n",
    "  - Ref: https://github.com/scikit-learn/scikit-learn/issues/12525 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columnTransformer and selector to extract feature names\n",
    "columnTransformer = featureSelector.named_steps['preProcess'].named_steps['columnTransform']\n",
    "selector = featureSelector.named_steps['selector']\n",
    "selectedFeatures = selector.get_support()\n",
    "featureNames = getFeatureNames(columnTransformer, selector)\n",
    "# Create dataframe of transformed data\n",
    "transformedDF = pd.DataFrame(\n",
    "    featureSelector.transform(X), \n",
    "    columns=featureNames)\n",
    "\n",
    "# Symetric difference between 'with' and 'without selection'\n",
    "allFeatures = set(getFeatureNames(columnTransformer))\n",
    "\n",
    "eliminatedFeatures = allFeatures ^ set(featureNames)\n",
    "print(f'Eliminated features: {eliminatedFeatures}')\n",
    "\n",
    "transformedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator hypertuning\n",
    "  - Build a fresh pipeline seperate from the feature selector.\n",
    "  - The custom FeatureFilter() transformer filters unselected features.\n",
    "  - We do this because we don't want to perform feature selection within a parameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FeatureFilter\n",
    " - In the previous step we created a seperate pipeline to performed pre-processing and recursive feature elimination to determine optimal features to pass to the model.\n",
    "   - This pipeline returns a boolean array of features selected.\n",
    " - The FeatureFilter transformer is included in the main pipeline to filter features before passing to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Filter columns by boolean mask \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullModel = Pipeline(steps=[\n",
    "    ('preProcess',    preProcessor),\n",
    "    ('featureFilter', FeatureFilter(selectedFeatures)),\n",
    "    ('model',         RandomForestClassifier(random_state=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ({\n",
    "    'preProcess__columnTransform__Age__discrete__n_bins':  range(2, 5),\n",
    "    'model__n_estimators':      range(100, 1000, 10),\n",
    "    'model__max_depth':         range(1, 20),\n",
    "    'model__criterion':         ['gini', 'entropy'],\n",
    "    'model__max_features':      range(1, len(featureNames) + 1),\n",
    "})\n",
    "\n",
    "gridSearch = RandomizedSearchCV(\n",
    "    fullModel, params, scoring='accuracy', random_state=1,\n",
    "    cv=cv, refit=True, n_jobs=nJobs, n_iter=100, verbose=1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "score = gridSearch.score(X_valid, y_valid)\n",
    "print(f'Best score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(testPath, index_col=index, dtype=dtypes)\n",
    "currentPredict = gridSearch.predict(X_test)\n",
    "\n",
    "correct = pd.read_csv('submissionTrue.csv')['Survived']\n",
    "myBest = pd.read_csv('submissionBest.csv')['Survived']\n",
    "actualScore = (correct == currentPredict).sum() / len(correct)\n",
    "print(actualScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesSearchCV\n",
    "  - Note: current version of skopt (0.8.1) not compatible with scikit-learn 0.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ({\n",
    "    'model__n_estimators':      Integer(100, 1000),\n",
    "    'model__max_depth':         Integer(3, 20),\n",
    "    'model__criterion':         Categorical(['gini', 'entropy']),\n",
    "    'model__max_features':      Integer(1, len(featureNames)),\n",
    "})\n",
    "\n",
    "gridSearch = BayesSearchCV(\n",
    "    fullModel, params, scoring='accuracy', random_state=1,\n",
    "    cv=cv, refit=True, n_jobs=nJobs, n_iter=50)\n",
    "gridSearch.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
